{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a5ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded successfully!\n",
      "              Open    High     Low   Close  Volume\n",
      "Date                                              \n",
      "2022-01-03  101.25  102.48   98.18  100.50   48712\n",
      "2022-01-04  100.82  103.05   98.19  100.36   43722\n",
      "2022-01-05  101.63  102.94  100.41  101.01   20625\n",
      "2022-01-06  102.87  104.43  102.33  102.55   11629\n",
      "2022-01-07  102.70  102.51  100.48  102.31   20973\n",
      "ðŸ“Š Training samples: 577 | Testing samples: 145\n",
      "ðŸš€ Training the model...\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kush2\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - loss: 0.0448 - val_loss: 0.0030\n",
      "Epoch 2/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0085 - val_loss: 0.0036\n",
      "Epoch 3/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0062 - val_loss: 0.0057\n",
      "Epoch 4/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0078 - val_loss: 0.0044\n",
      "Epoch 5/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0065 - val_loss: 0.0050\n",
      "Epoch 6/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0052 - val_loss: 0.0034\n",
      "Epoch 7/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0052 - val_loss: 0.0041\n",
      "Epoch 8/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0053 - val_loss: 0.0027\n",
      "Epoch 9/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0051 - val_loss: 0.0027\n",
      "Epoch 10/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0045 - val_loss: 0.0030\n",
      "Epoch 11/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0046 - val_loss: 0.0034\n",
      "Epoch 12/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 13/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0047 - val_loss: 0.0049\n",
      "Epoch 14/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0046 - val_loss: 0.0026\n",
      "Epoch 15/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0041 - val_loss: 0.0027\n",
      "Epoch 16/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0044 - val_loss: 0.0027\n",
      "Epoch 17/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0048 - val_loss: 0.0023\n",
      "Epoch 18/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0043 - val_loss: 0.0024\n",
      "Epoch 19/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0041 - val_loss: 0.0027\n",
      "Epoch 20/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0042 - val_loss: 0.0023\n",
      "Epoch 21/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0043 - val_loss: 0.0020\n",
      "Epoch 22/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0065 - val_loss: 0.0021\n",
      "Epoch 23/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0047 - val_loss: 0.0022\n",
      "Epoch 24/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0045 - val_loss: 0.0036\n",
      "Epoch 25/25\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0039 - val_loss: 0.0028\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Sample Predicted Trends:\n",
      "Day 1: ['Down']\n",
      "Day 2: ['Down']\n",
      "Day 3: ['Down']\n",
      "Day 4: ['Down']\n",
      "Day 5: ['Down']\n",
      "Day 6: ['Down']\n",
      "Day 7: ['Down']\n",
      "Day 8: ['Down']\n",
      "Day 9: ['Down']\n",
      "Day 10: ['Down']\n",
      "\n",
      "âœ… Model saved as 'trend_model.h5'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "df = pd.read_csv('historical_data.csv')\n",
    "\n",
    "\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])  # Converts the Date column (which is text) into proper datetime objects, so Python understands it as dates.\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "print(\"âœ… Dataset loaded successfully!\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# Use only the 'Close' price for trend prediction\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df[['Close']].values) # Takes only the â€œCloseâ€ prices and scales them between 0 and 1.\n",
    "\n",
    "# Function to create training sequences\n",
    "def create_sequences(data, seq_len):\n",
    "    X, y = [], []\n",
    "    for i in range(seq_len, len(data)):\n",
    "        X.append(data[i-seq_len:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 60  # last 60 days â†’ next day prediction\n",
    "X, y = create_sequences(scaled_data, sequence_length)\n",
    "\n",
    "# Split into train/test (80/20)\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"ðŸ“Š Training samples: {len(X_train)} | Testing samples: {len(X_test)}\")\n",
    "\n",
    "# =========================\n",
    "#  STEP 3 â€” BUILD LSTM MODEL\n",
    "# =========================\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# =========================\n",
    "#  STEP 4 â€” TRAIN MODEL\n",
    "# =========================\n",
    "print(\"ðŸš€ Training the model...\")\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# =========================\n",
    "#  STEP 5 â€” PREDICT & FIND TREND\n",
    "# =========================\n",
    "predicted = model.predict(X_test)\n",
    "predicted_prices = scaler.inverse_transform(predicted)\n",
    "real_prices = scaler.inverse_transform(y_test)\n",
    "\n",
    "# Convert to trend labels\n",
    "trend = np.where(predicted_prices[1:] > predicted_prices[:-1], 'Up', 'Down')\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"\\nðŸ“ˆ Sample Predicted Trends:\")\n",
    "for i in range(10):\n",
    "    print(f\"Day {i+1}: {trend[i]}\")\n",
    "\n",
    "# =========================\n",
    "#  STEP 6 â€” (Optional) SAVE MODEL\n",
    "# =========================\n",
    "model.save('trend_model.h5')\n",
    "print(\"\\nâœ… Model saved as 'trend_model.h5'\")\n",
    "\n",
    "\n",
    "\n",
    "# ['Up']\tThe model predicts that tomorrowâ€™s closing price will be higher than todayâ€™s â€” an upward trend.\n",
    "# ['Down']\tThe model predicts that tomorrowâ€™s closing price will be lower than todayâ€™s â€” a downward trend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7528558c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
