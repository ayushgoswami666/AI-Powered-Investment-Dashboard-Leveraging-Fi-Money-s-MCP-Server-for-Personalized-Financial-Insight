{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87a30080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded successfully!\n",
      "              Open    High     Low   Close   Volume\n",
      "Date                                               \n",
      "2025-01-01  100.00  101.58   99.56  101.55  4718635\n",
      "2025-01-02  101.55  101.64  100.16  100.84  2670478\n",
      "2025-01-03  100.84  102.43  100.79  101.51  4840580\n",
      "2025-01-04  101.51  102.68  101.18  102.36  2830051\n",
      "2025-01-05  102.36  102.40  100.81  101.80  1654576\n",
      "ðŸ“Š Total sequences created: 127\n",
      "ðŸ“Œ Train Size: 101 | Test Size: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kush2\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training Started...\n",
      "\n",
      "Epoch 1/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 422ms/step - loss: 0.3328 - val_loss: 0.0536\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - loss: 0.0970 - val_loss: 0.0261\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step - loss: 0.0771 - val_loss: 0.0021\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - loss: 0.0558 - val_loss: 0.0312\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - loss: 0.0246 - val_loss: 0.0405\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - loss: 0.0210 - val_loss: 0.0706\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - loss: 0.0140 - val_loss: 0.0723\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step - loss: 0.0120 - val_loss: 0.0702\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - loss: 0.0138 - val_loss: 0.0713\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - loss: 0.0076 - val_loss: 0.0774\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 758ms/step\n",
      "\n",
      "ðŸ“ˆ Trend Output Sample:\n",
      "Day 1: Down\n",
      "Day 2: Up\n",
      "Day 3: Up\n",
      "Day 4: Up\n",
      "Day 5: Up\n",
      "Day 6: Down\n",
      "Day 7: Up\n",
      "Day 8: Down\n",
      "Day 9: Down\n",
      "Day 10: Up\n",
      "\n",
      "âœ… Model saved as trend_model.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 772ms/step\n",
      "\n",
      "ðŸ”® Tomorrow Trend: Down\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, LayerNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# =========================\n",
    "#  STEP 1 â€” LOAD DATASET\n",
    "# =========================\n",
    "df = pd.read_csv('historical_data.csv')\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "print(\"âœ… Dataset loaded successfully!\")\n",
    "print(df.head())\n",
    "\n",
    "# =========================\n",
    "#  STEP 2 â€” FEATURE ENGINEERING\n",
    "# =========================\n",
    "df['High-Low'] = df['High'] - df['Low']\n",
    "df['Open-Close'] = df['Open'] - df['Close']\n",
    "df['7day MA'] = df['Close'].rolling(7).mean()\n",
    "df['14day MA'] = df['Close'].rolling(14).mean()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "features = df[['Close', 'Volume', 'High-Low', 'Open-Close', '7day MA', '14day MA']].values\n",
    "\n",
    "# =========================\n",
    "#  STEP 3 â€” SCALE FEATURES\n",
    "# =========================\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(features)\n",
    "\n",
    "# =========================\n",
    "#  STEP 4 â€” CREATE SEQUENCES\n",
    "# =========================\n",
    "def create_sequences(data, seq_len):\n",
    "    X, y = [], []\n",
    "    for i in range(seq_len, len(data)):\n",
    "        X.append(data[i-seq_len:i])\n",
    "        y.append(data[i, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_len = 60  \n",
    "X, y = create_sequences(scaled_data, seq_len)\n",
    "\n",
    "print(f\"ðŸ“Š Total sequences created: {len(X)}\")\n",
    "\n",
    "# Train/Test split\n",
    "train = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train], X[train:]\n",
    "y_train, y_test = y[:train], y[train:]\n",
    "\n",
    "print(f\"ðŸ“Œ Train Size: {len(X_train)} | Test Size: {len(X_test)}\")\n",
    "\n",
    "# =========================\n",
    "#  STEP 5 â€” BUILD LSTM MODEL\n",
    "# =========================\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(128, return_sequences=True), input_shape=(seq_len, X.shape[2])),\n",
    "    LayerNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    LayerNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Bidirectional(LSTM(64)),\n",
    "    LayerNormalization(),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0005, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss='huber')\n",
    "\n",
    "# Callbacks\n",
    "stopper = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "saver = ModelCheckpoint(\"best_model.keras\", monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# =========================\n",
    "#  STEP 6 â€” TRAIN MODEL\n",
    "# =========================\n",
    "print(\"\\nðŸš€ Training Started...\\n\")\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[stopper, saver]\n",
    ")\n",
    "\n",
    "# =========================\n",
    "#  STEP 7 â€” LOAD BEST MODEL\n",
    "# =========================\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"best_model.keras\")\n",
    "\n",
    "# =========================\n",
    "#  STEP 8 â€” PREDICT & GENERATE TREND\n",
    "# =========================\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "dummy = np.zeros((len(pred), scaled_data.shape[1]))\n",
    "dummy[:, 0] = pred[:, 0]\n",
    "\n",
    "pred_prices = scaler.inverse_transform(dummy)[:, 0]\n",
    "\n",
    "trend = np.where(pred_prices[1:] > pred_prices[:-1], \"Up\", \"Down\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Trend Output Sample:\")\n",
    "for i in range(10):\n",
    "    print(f\"Day {i+1}: {trend[i]}\")\n",
    "\n",
    "# Save final trained model\n",
    "model.save(\"trend_model.keras\")\n",
    "print(\"\\nâœ… Model saved as trend_model.keras\")\n",
    "\n",
    "# =========================\n",
    "#  STEP 9 â€” FUTURE TREND FUNCTION\n",
    "# =========================\n",
    "def predict_next_trend(model, df, data, seq_len=60):\n",
    "    X_input = data[-seq_len:].reshape(1, seq_len, data.shape[1])\n",
    "    pred_price = model.predict(X_input)\n",
    "\n",
    "    dummy = np.zeros((1, data.shape[1]))\n",
    "    dummy[:, 0] = pred_price\n",
    "    next_price = scaler.inverse_transform(dummy)[0, 0]\n",
    "\n",
    "    today = df['Close'].iloc[-1]\n",
    "    return \"Up\" if next_price > today else \"Down\"\n",
    "\n",
    "next_trend = predict_next_trend(model, df, scaled_data)\n",
    "print(\"\\nðŸ”® Tomorrow Trend:\", next_trend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc632c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
